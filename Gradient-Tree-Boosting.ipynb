{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3379a103",
   "metadata": {},
   "source": [
    "#### Autor: Kevin Cioch\n",
    "\n",
    "# Einführung Machine Learning - SS23\n",
    "## Ensemble Classifier - Gradient Tree Boosting\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7367352b",
   "metadata": {},
   "source": [
    "## Einleitung\n",
    "\n",
    "Ensemble-Classifier sind innerhalb des Machine Learnings ein leistungsstarkes Verfahren zur Klassifikation von Daten, bei dem mehrere einzelne Klassifikatoren oder Vorhersagemodelle zu einem einzigen Modell kombiniert werden. Die Idee dahinter beruht auf dem Konzept, dass durch die Kombination unterschiedlicher Modelle die Stärken der einzelnen Klassifikatoren genutzt und die Schwächen durch das Kollektiv an verschiedenen Klassifikatoren ausgeglichen werden. Dadurch wird letztendlich die Vorhersagegenauigkeit verbessert und das finale Vorhersagemodell robuster gegenüber individuellen Fehlern und Schwächen einzelner Modelle.\n",
    "\n",
    "Grundsätzlich kann man Ensemble Classifier in zwei Kategorien unterteilen. Zum einem gibt es **Bagging-basierte** Ensemble-Methoden, die auf dem Prinzip beruhen, dass mehrere Klassifikatoren unabhängig voneinander trainiert werden und der Durchschnitt aller Modelle für eine Schätzung berechnet wird, z.B. Bagging Methoden oder Random Forest. Zum anderen gibt es **Boosting-basierte** Ensemble-Methoden, die das Konzept verfolgen mehrere Klassifikatoren sequenziell zu trainieren und dabei mit jeder Iteration die Fehler des Vorgängers zu reduzieren, z.B. AdaBoost oder Gradient Tree Boosting.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ce3fc0d",
   "metadata": {},
   "source": [
    "## Gradient Tree Boosting\n",
    "\n",
    "Das Gradient Tree Boosting Machine Learning Verfahren erweitert das Konzept von herkömmlichen Decision Trees um die Idee des Boostings als Ensemble Methode. Dabei werden **G**radient **B**oosted **D**ecision **T**ree**s** (GBDTs) aus mehreren einzelnen Decision Tree konstruiert, die sequenziell abhängig voneinander konstruiert werden und bei jeder Iteration die Residien (Fehler) der vorherigen Iteration korrigieren. Das Verfahren nutzt dabei den Gradientenabstieg um mit jeder weiteren Iteration die Vorhersagegenauigkeit kontinuierlich weiter zu verbessern.\n",
    "\n",
    "### Anwendungsfälle\n",
    "\n",
    "Das Gradient Tree Boosting-Verfahren ist vielseitig in der Praxis einsetzbar und bietet einen guten Lösungsansatz für unterschiedliche Problembstellungen in verschiedenen Bereichen des maschinellen Lernens und der Datenanalyse. Zu klassischen Klassifikationsaufgaben wie Spam-Erkennung oder Bilderkennung und Regressionsaufgaben wie Preisvorhersagen bietet der Algorithmus auch eine Lösungsmöglichkeit für Anomalieerkennungen und Rangfolgenbestimmungen wie Suchmaschinen-Rankings sowie die Bewertung von Features.\n",
    "\n",
    "### Boosting\n",
    "\n",
    "Das Boosting-Verfahren folgt folgenden Schritten:\n",
    "\n",
    "1. Erstellen eines Basismodells.\n",
    "2. Berechnung der Residien (Unterschiede zwischen den tatsächlichen Zielwerten und den Vorhersagen des aktuellen Modells)\n",
    "3. Konstruktion des nächsten Entscheidungsbaums: In jeder Iteration wird ein weiterer Entscheidungsbaum erstellt, der darauf abzielt, die Residuen des vorherigen Modells zu reduzieren. Der Baum wird so erstellt, dass er die verbleibenden Fehler des Modells möglichst effektiv korrigiert. Hierbei wird ein Gradientenabstiegsverfahren angewendet, um die Richtung zu bestimmen, in die der Baum wachsen soll.\n",
    "4. Aktualisierung des Modells: Der neu erstellte Entscheidungsbaum wird dem Modell hinzugefügt und das Modell wird aktualisiert, indem die Vorhersagen des neuen Baums zum bisherigen Modell addiert werden. Dadurch wird das Modell schrittweise verbessert, da es die Fehler des vorherigen Modells korrigiert und sich an die Trainingsdaten anpasst.\n",
    "5. Wiederholung der Schritte 2-4: Die Schritte der Berechnung der Residuen, der Erstellung eines neuen Entscheidungsbaums und der Aktualisierung des Modells werden wiederholt, bis ein vordefiniertes Kriterium erreicht ist. Dieses Kriterium kann die maximale Anzahl von Iterationen, eine vorgegebene Fehlergrenze oder andere Leistungsindikatoren sein.\n",
    "\n",
    "Durch die Kombination mehrerer Entscheidungsbäume, die in aufeinanderfolgenden Iterationen erstellt werden, wird das Modell kontinuierlich verbessert. Jeder Baum konzentriert sich auf die Korrektur der Fehler, die in den vorherigen Iterationen gemacht wurden. Dadurch wird eine insgesamt bessere Vorhersagegenauigkeit erzielt.\n",
    "\n",
    "> Schritt 5. zeigt die Gefahr von Decision Trees / Gradient Boosting Trees auf! Bei zu vielen Iteration (zu hohe Baumtiefe) neigt das Vorhersagemodell schnell zu einem Overfitting an die Trainingsdaten. Gut zu beobachten ist dieses Verhalten bei der Analyse der Lernkurven für Trainings- und Testdaten, die bei einem Overfitting stark voneinander abweichen. Daher ist es ratsam Regularisierungsmethoden zu verwenden, z.B. Festlegen der Hyperparameter *max_depth*, *n_estimators*, *max_leaf_nodes* oder *learning_rate*.\n",
    "\n",
    "### Vorteile\n",
    "\n",
    "- **Leistungstarkes Modell:** GBDTs sind leistungstarke Ensemble-Modelle, die eine hohe Vorhersagegenauigkeit aufweisen können und komplexe nichtlineare Zusammenhänge in Daten erfassen und sowohl für Klassifikations- als auch für Regressionsprobleme eingesetzt werden können.\n",
    "- **Interpretierbarkeit:** Auch wenn GBDTs komplexer als einfache Decision Trees sind, können die Ergebnisse der Vorhersagen aufgrund der Baumstruktur anders als bei Blackbox-artigen ML Algorithmen nachvollzogen werden.\n",
    "\n",
    "### Nachteile\n",
    "\n",
    "- **Trainingszeit:** Da die einzelnen Bäume nacheinander trainiert werden müssen, da sie aufeinander aufbauen und sich auf Grundlage der Fehler der Vorgänger anpassen, und die Berechnung der Gradienten der Verlustfunktion bei vielen Features recht viel Zeit beansprucht, kann der Trainingsaufwand recht intensiv werden.\n",
    "- **Anfällig für Overfitting:** Decision Trees neigen grundsätzlich bei komplexen Baumstrukturen zu einer ausgeprägten Anpassung an die Trainingsdaten. Es ist daher wichtig Regularisierungsmethoden einzusetzen wie das Festlegen der maximalen Baum-Tiefe oder die minimale Anzahl der Datenpunkte in Blättern / Knoten zu begrenzen.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a28454b",
   "metadata": {},
   "source": [
    "## SciKit-Learn\n",
    "\n",
    "Für das Gradient Tree Boosting-Verfahren ist es möglich, den **DecisionTreeClassifier** der Scikit-learn Bibliothek manuell zu verwenden und diese Entscheidungsbäume miteinander zu verbinden. Indem man den DecisionTreeClassifier in einer Schleife iterativ trainiert und die Vorhersagen des vorherigen Baums zu den Residuen des nächsten Baums hinzufügt, kann man ein eigenes Gradient Boosting-Modell erstellen. Dies erfordert jedoch eine sorgfältige Handhabung der Hyperparameter und eine manuelle Implementierung des Boosting-Prozesses.\n",
    "\n",
    "Es ist daher einfacher den Gradient Boosted Decision Tree direkt über die **GradientBoostingClassifier** Klasse der Scikit-learn Bibliothek zu implementieren. Diese bietet die Möglichkeit über Parameter wie _max_depth_, die maximale Baumtiefe, _n_estimator_, die Anzahl der Bäume oder _learning_rate_, die Lernrate zu beeinflussen.\n",
    "\n",
    "> Mit Scikit-learn 0.21 wurden zwei neue Implementierungen von GBDTs eingeführt, **HistGradientBoostingClassifiert** und **HistGradientBoostingRegressor**. Diese histogrammbasierten Algorithmen können bei mehr als zehntausend Datenpunkten deutlich perfomanter ausfallen als die herkömmlichen GBDTs. Zudem bieten sie eine Unterstützung für fehlende Werte, sodass kein Imputer benötigt wird.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32adb709",
   "metadata": {},
   "source": [
    "## Paxisbeispiel\n",
    "\n",
    "Im folgenden Praxisbeispiel wird die Implementierung des Gradient Tree Boosting mit der Scikit-learn Bibliothek veranschaulicht. Die Problemstellung bezieht sich auf die Klassifikation von Diabetes anhand von Parametern aus einem Patientdatensatz [kaggle]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc429ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World!\")\n",
    "print(2**2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53d18e67",
   "metadata": {},
   "source": [
    "## Quellenverzeichnis\n",
    "\n",
    "1. Gradient Tree Boosting. 25 Mai 2023.\\\n",
    "   [https://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting](https://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting)\n",
    "\n",
    "2. sklearn.ensemble.GradientBoostingClassifier. 26 Mai 2023.\\\n",
    "    [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\n",
    "\n",
    "3. Praxiseinstieg Machine Learning mit Scikit-Learn, Keras und Tensorflow: Konzepte, Tools und Techniken für intelligente Systeme [2. Auflage]. Géron, A. 2020. dpunkt.verlag "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
