{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3379a103",
   "metadata": {},
   "source": [
    "#### Autor: Kevin Cioch\n",
    "\n",
    "# Einführung Machine Learning - SS23\n",
    "## Ensemble Classifier - Gradient Tree Boosting\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7367352b",
   "metadata": {},
   "source": [
    "## Einleitung"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ce3fc0d",
   "metadata": {},
   "source": [
    "## Gradient Tree Boosting\n",
    "\n",
    "### Anwendungsfälle\n",
    "\n",
    "G\n",
    "\n",
    "### Boosting\n",
    "\n",
    "1. Erstellen eines Inital-Modells\n",
    "2. Vorhersagen für den gesamten Datensatz durchführen\n",
    "3. Berechnen des Fehlers anhand der Vorhersagen und der tatsächlichen Werte\n",
    "4. Falschen Vorhersagen mehr Gewicht zuweisen\n",
    "5. Erstellen Sie ein weiteres Modell, das versucht, Fehler des letzten Modells zu beheben.\n",
    "6. Mit dem neuen Modell Vorhersagen für den gesamten Datensatz durchführen\n",
    "7. Erstellen von mehreren Modellen, wobei jedes Modell darauf abzielt, die vom vorherigen Modell verursachten Fehler zu korrigieren\n",
    "8. Endgültiges Modell erzeugen durch das Bilden des Mittelwertes aller Modelle.\n",
    "\n",
    "### Vorteile\n",
    "\n",
    "- **Leistungstarkes Modell:** GBDTs sind leistungstarke Ensemble-Modelle, die eine hohe Vorhersagegenauigkeit aufweisen können und komplexe nichtlineare Zusammenhänge in Daten erfassen und sowohl für Klassifikations- als auch für Regressionsprobleme eingesetzt werden können.\n",
    "- **Interpretierbarkeit:** Auch wenn GBDTs komplexer als einfache Decision Trees sind, können die Ergebnisse der Vorhersagen aufgrund der Baumstruktur anders als bei Blackbox-artigen ML Algorithemn nachvollzogen werden.\n",
    "\n",
    "### Nachteile\n",
    "\n",
    "- **Trainingszeit:** Da die einzelnen Bäume nacheinander trainiert werden müssen, da sie aufeinander aufbauen und sich auf grundlage der Fehler der Vorgänger anpassen, und die Berechnung der Gradienten der Verlustfunktion bei vielen Features recht viel Zeit beansprucht, kann der Trainingsaufwand recht intensiv werden.\n",
    "- **Anfällig für Overfitting:** Decision Trees neigen grundsätzlich bei komplexen Baumstrukturen zu einer ausgeprägten Anpassung an die Trainingsdaten. Es ist daher wichtig Regularisierungsmethoden einzusetzen wie das festlegen der maximalen Baum-Tiefe oder die minimale Anzahl der Datenpunkte in Blättern / Knoten zu begrenzen.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a28454b",
   "metadata": {},
   "source": [
    "## SciKit-Learn\n",
    "\n",
    "Für das Gradient Tree Boosting-Verfahren ist es möglich, den **DecisionTreeClassifier** der scikit-learn Bibliothek manuell zu verwenden und diese Entscheidungsbäume miteinander zu verbinden. Indem man den DecisionTreeClassifier in einer Schleife iterativ trainiert und die Vorhersagen des vorherigen Baums zu den Residuen des nächsten Baums hinzufügt, kann man ein eigenes Gradient Boosting-Modell erstellen. Dies erfordert jedoch eine sorgfältige Handhabung der Hyperparameter und eine manuelle Implementierung des Boosting-Prozesses.\n",
    "\n",
    "Es ist daher einfacher den Gradient Boosted Decision Tree direkt über die **GradientBoostingClassifier** Klasse der scikit-learn Bibliothek zu implementieren. Diese bietet die Möglichkeit über Parameter wie *max_depth*, die maximale Baumtiefe, *n_estimator*, die Anzahl der Bäume oder *learnin_rate*, die Lernrate zu beeinflussen.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32adb709",
   "metadata": {},
   "source": [
    "## Paxisbeispiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc429ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World!\")\n",
    "print(2**2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53d18e67",
   "metadata": {},
   "source": [
    "## Quellenverzeichnis\n",
    "\n",
    "1. Gradient Tree Boosting. 25 Mai 2023.\\\n",
    "   [https://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting](https://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting)\n",
    "\n",
    "2. sklearn.ensemble.GradientBoostingClassifier. 26 Mai 2023.\\\n",
    "    [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\n",
    "\n",
    "3. Praxiseinstieg Machine Learning mit Scikit-Learn, Keras und Tensorflow: Konzepte, Tools und Techniken für intelligente Systeme [2. Auflage]. Géron, A. 2020. dpunkt.verlag "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
